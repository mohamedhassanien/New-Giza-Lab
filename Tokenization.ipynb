{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Tokenization in Natural Language Processing (NLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: By the end of this lab, you will understand the concept of tokenization in NLP, different types of tokenization  and apply hands-on techniques to tokenize text using Python libraries such as nltk, regex, and Hugging Face's transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Tokenization \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Tokenization Matters:\n",
    "\n",
    "- Enables machine learning models to interpret text.\n",
    "\n",
    "- Necessary for downstream tasks like text classification sentiment analysis, and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word Tokenization: Splitting text into words.\n",
    "    - Example:  \"I love NLP\" ->['I', 'love', 'NLP']\n",
    "\n",
    "2. Subword Tokenization: Splitting text into subword units, often used in modern NLP models.\n",
    "    - Example:  \"playing\" -> ['play', '##ing']\n",
    "\n",
    "3. Character Tokenization: Splitting text into individual characters.\n",
    "    - Example: \"NLP\" -> ['N', 'L', 'P']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization with Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- \\w: Matches any alphanumeric character (equivalent to [a-zA-Z0-9_])\n",
    "\n",
    "- \\W: Matches any non-alphanumeric character\n",
    "\n",
    "- \\s: Matches any whitespace character\n",
    "\n",
    "- \\S: Matches any non-whitespace character\n",
    "\n",
    "- \\d: Matches any digit character\n",
    "\n",
    "- \\D: Matches any non-digit character\n",
    "\n",
    "- \\w{N}: Matches any alphanumeric N characters\n",
    "\n",
    "- +: One or more of the preceding expression\n",
    "\n",
    "- *: Zero or more of the preceding expression\n",
    "- | : Or operator\n",
    "- []: Used to match cases , [A-Z] will get you any uppercase letter\n",
    "- (): Used for grouping , (A-Z) will get you only any string or substring equal to (A-Z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex Tokens: ['Tokenization', 'is', 'crucial', 'for', 'NLP', '!', 'Let', \"'\", 's', 'tokenize', 'this', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Simple text for tokenization\n",
    "text = \"Tokenization is crucial for NLP! Let's tokenize this text.\"\n",
    "\n",
    "# Tokenizing using regex (split on spaces and punctuation)\n",
    "tokens = re.findall(r'\\w+|\\S', text)\n",
    "print(\"Regex Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags: ['#AI', '#Innovation']\n",
      "Mentions: ['@OpenAI']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample tweet\n",
    "tweet = \"Loving the new features of #AI in 2024! Thanks @OpenAI for the great work. #Innovation\"\n",
    "\n",
    "# Tokenize hashtags and mentions\n",
    "hashtags = re.findall(r'#\\w+', tweet)\n",
    "mentions = re.findall(r'@\\w+', tweet)\n",
    "\n",
    "print(\"Hashtags:\", hashtags)\n",
    "print(\"Mentions:\", mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'Is', 'Fun']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample CamelCase text\n",
    "camel_case_text = \"NaturalLanguageProcessingIsFun\"\n",
    "\n",
    "# Regex to split CamelCase words\n",
    "words = re.findall(r'[A-Z][a-z]*', camel_case_text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenize Sentences\n",
    "\n",
    "### Write a regex pattern to tokenize a paragraph into individual sentences.\n",
    "\n",
    "### Task: Use regex to split the following text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello world! How are you doing today? It's a sunny day. Let's enjoy it.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 : Tokenize words including Apostrophes\n",
    "### Write a regex pattern to extract words from a sentence while retaining contractions and words with apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"It's a beautiful day, don't you think?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Special Case Tokenization\n",
    "\n",
    "### Write a function to detect and separate URLs and emails during tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Visit us at https://example.com or email info@example.com.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 4.1   word tokenization with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mohamedhassanien/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Tokenization', 'is', 'essential', 'for', 'text', 'processing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "# Download the punkt package\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Tokenization is essential for text processing.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Word Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 4.2 Sentence Tokenization with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Tokenization is crucial in NLP.', 'It helps machines understand language.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "paragraph = \"Tokenization is crucial in NLP. It helps machines understand language.\"\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(\"Sentences:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Sentence tokenization\n",
    "### Tokenize the paragraph into individual sentences, observing how NLTK deals with abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Dr. Smith is a renowned scientist. He earned his Ph.D. in AI in 2020. Can you believe that?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 : Word Tokenization\n",
    "\n",
    "### Tokenize this sentence into individual words using the word_tokenize function from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"NLTK is a powerful Python library for natural language processing!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Word tokenize\n",
    "\n",
    "### Tokenize a sentence into words, remove punctuation, and filter out stop words.\n",
    "\n",
    "### Hint: use from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox, jumps over a lazy dog!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Subword Tokenization with Byte Pair Encoding (BPE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedhassanien/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mohamedhassanien/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword Tokens: ['token', '##ization', 'is', 'a', 'critical', 'step', 'for', 'language', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Tokenization is a critical step for language models.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Subword Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 : BPE\n",
    "\n",
    "### Try tokenizing this sentence: \"Deep learning models require tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Tokenization Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ambiguity: Words like \"don't\" could be split into [\"do\", \"n't\"] depending on the tokenizer. Try asking chatgpt 4o how many 'r' in strawberry\n",
    "\n",
    "- Languages: Tokenization rules differ by language (e.g., arabic tokenization is more complex).\n",
    "\n",
    "- Subword vs. Word: The choice of tokenization can significantly affect the performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
